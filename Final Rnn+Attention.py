# -*- coding: utf-8 -*-
"""Capstone Final RNN+Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_fihBac7fymPKTpGN9729WjDBBi7R8x6
"""

import os
import zipfile
import glob
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, GRU, Dense, Dropout, Layer, Bidirectional
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from music21 import note, chord, stream
from google.colab import files
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import Audio, display, HTML
from tqdm import tqdm
import multiprocessing
import random
import io
from collections import Counter
import pandas as pd

print("Installing required packages...")
os.system("pip install mido -q")
os.system("pip install midi2audio -q")
os.system("apt-get update -qq && apt-get install -qq fluidsynth")
import mido

if tf.config.list_physical_devices('GPU'):
    print("GPU available. Using mixed precision training...")
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
else:
    print("GPU not available. Using standard precision.")

print("Setup completed successfully!")

ZIP_FILE_PATH = '/MIDI Dataset Capstone.zip'
EXTRACT_PATH = './dataset'

def unzip_dataset(zip_path, extract_path=EXTRACT_PATH):
    print(f"Unzipping dataset from {zip_path} to {extract_path}...")

    if not os.path.exists(extract_path):
        os.makedirs(extract_path)

    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

    print("Dataset unzipped successfully!")
    return extract_path

if os.path.exists(ZIP_FILE_PATH):
    print(f"Using dataset from existing path: {ZIP_FILE_PATH}")
    zip_path = ZIP_FILE_PATH
else:
    print(f"Dataset not found at {ZIP_FILE_PATH}. Please upload the ZIP file.")
    uploaded = files.upload()

    if not uploaded:
        print("No file was uploaded. Exiting.")
        raise Exception("No file uploaded")

    zip_path = list(uploaded.keys())[0]

dataset_path = unzip_dataset(zip_path)

midi_files = []
for root, _, _ in os.walk(dataset_path):
    midi_files.extend(glob.glob(os.path.join(root, "*.mid")))

if not midi_files:
    for root, _, _ in os.walk(dataset_path):
        midi_files.extend(glob.glob(os.path.join(root, "*.midi")))

print(f"Found {len(midi_files)} MIDI files.")

composers = [os.path.basename(os.path.dirname(f)) for f in midi_files]
composer_counts = Counter(composers)

plt.figure(figsize=(12, 6))
sns.barplot(x=list(composer_counts.keys()), y=list(composer_counts.values()))
plt.title('MIDI Files per Composer')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Number of Files')
plt.tight_layout()
plt.show()

def process_midi_files(dataset_path):
    print("Processing MIDI files...")

    midi_files = []
    for root, _, _ in os.walk(dataset_path):
        midi_files.extend(glob.glob(os.path.join(root, "*.mid")))

    if not midi_files:
        for root, _, _ in os.walk(dataset_path):
            midi_files.extend(glob.glob(os.path.join(root, "*.midi")))

    print(f"Processing {len(midi_files)} MIDI files.")

    all_notes = []
    skipped_files = 0
    processed_files = 0
    total_tracks = 0
    skipped_tracks = 0
    composer_notes = {}

    num_cores = multiprocessing.cpu_count()
    batch_size = max(1, len(midi_files) // num_cores)

    for i in tqdm(range(0, len(midi_files), batch_size), desc="Processing batches"):
        batch = midi_files[i:i+batch_size]
        for file in batch:
            try:
                composer = os.path.basename(os.path.dirname(file))
                print(f"Processing {file}...")

                mid = mido.MidiFile(file)
                file_notes = []
                file_tracks = 0
                file_skipped = 0

                for track_idx, track in enumerate(mid.tracks):
                    track_name = None
                    for msg in track:
                        if msg.type == 'track_name':
                            track_name = msg.name
                            break

                    if track_name and ('copyright' in track_name.lower() or 'Â©' in track_name):
                        print(f"  - Skipping copyright track: {track_name}")
                        file_skipped += 1
                        skipped_tracks += 1
                        continue

                    notes_in_track = []
                    active_notes = {}

                    for msg in track:
                        if msg.type == 'note_on' and msg.velocity > 0:
                            active_notes[msg.note] = msg.time
                        elif (msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0)) and msg.note in active_notes:
                            pitch = msg.note
                            notes_in_track.append(pitch)
                            del active_notes[msg.note]

                    if notes_in_track:
                        file_tracks += 1
                        total_tracks += 1
                        print(f"  - Track {track_idx}: Extracted {len(notes_in_track)} notes")
                        file_notes.extend([str(n) for n in notes_in_track])

                if file_notes:
                    if composer not in composer_notes:
                        composer_notes[composer] = []
                    composer_notes[composer].extend(file_notes)

                    all_notes.extend(file_notes)
                    processed_files += 1
                    print(f"  - Total: {len(file_notes)} notes from {file_tracks} tracks ({file_skipped} skipped)")
                else:
                    print(f"  - No notes extracted from file")
                    skipped_files += 1

            except Exception as e:
                print(f"Error processing {file}: {e}")
                skipped_files += 1
                continue

    print(f"Processing complete:")
    print(f"- Extracted {len(all_notes)} total notes")
    print(f"- Processed {processed_files} files successfully")
    print(f"- Skipped {skipped_files} files due to errors")
    print(f"- Used {total_tracks} tracks for extraction")
    print(f"- Skipped {skipped_tracks} copyright tracks")

    note_counts = Counter(all_notes)
    top_notes = dict(note_counts.most_common(20))

    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.bar(top_notes.keys(), top_notes.values())
    plt.title('20 Most Common Notes')
    plt.xlabel('MIDI Note')
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)

    plt.subplot(1, 2, 2)
    plt.hist([int(n) for n in all_notes], bins=30, color='skyblue', edgecolor='black')
    plt.title('Distribution of Notes')
    plt.xlabel('MIDI Note Value')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

    composer_note_counts = {composer: len(notes) for composer, notes in composer_notes.items()}
    plt.figure(figsize=(12, 6))
    plt.bar(composer_note_counts.keys(), composer_note_counts.values())
    plt.title('Notes Extracted per Composer')
    plt.xlabel('Composer')
    plt.ylabel('Number of Notes')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    return all_notes

notes = process_midi_files(dataset_path)

if not notes:
    print("No notes were extracted. Check the MIDI files and try again.")
    raise Exception("No notes extracted")

print(f"Total notes extracted: {len(notes)}")

def prepare_sequences(notes, sequence_length=100):
    print("Preparing sequences...")

    pitchnames = sorted(set(notes))
    print(f"Number of unique notes: {len(pitchnames)}")

    note_to_int = {note: i for i, note in enumerate(pitchnames)}
    int_to_note = {i: note for i, note in enumerate(pitchnames)}

    x = []
    y = []

    for i in tqdm(range(0, len(notes) - sequence_length, 1), desc="Creating sequences"):
        input_sequence = notes[i:i + sequence_length]
        output_note = notes[i + sequence_length]

        x.append([note_to_int[note] for note in input_sequence])
        y.append(note_to_int[output_note])

    x = np.array(x)
    x = x / float(len(pitchnames))

    y = tf.keras.utils.to_categorical(y, num_classes=len(pitchnames))

    print(f"Created {len(x)} input sequences and {len(y)} output sequences")
    print(f"Input shape: {x.shape}, Output shape: {y.shape}")

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    sample_idx = np.random.randint(0, len(x))
    plt.plot(x[sample_idx])
    plt.title(f'Sample Input Sequence (Normalized)')
    plt.xlabel('Sequence Position')
    plt.ylabel('Note Value (Normalized)')

    plt.subplot(1, 2, 2)
    note_distribution = [np.argmax(y_i) for y_i in y[:1000]]  # Sample of 1000 output notes
    plt.hist(note_distribution, bins=min(50, len(pitchnames)))
    plt.title('Distribution of Target Notes')
    plt.xlabel('Note Index')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

    print("Sequence generation statistics:")
    print(f"- Number of unique notes/chords: {len(pitchnames)}")
    print(f"- Sequence length: {sequence_length}")
    print(f"- Total sequences: {len(x)}")

    return x, y, int_to_note, note_to_int, pitchnames

sequence_length = 100
x, y, int_to_note, note_to_int, pitchnames = prepare_sequences(notes, sequence_length)

x = x.reshape(x.shape[0], x.shape[1], 1)
print(f"Reshaped input data to {x.shape} for model input")

data_to_save = {
    'notes': notes,
    'note_to_int': note_to_int,
    'int_to_note': int_to_note,
    'pitchnames': pitchnames,
    'sequence_length': sequence_length
}

np.save('generation_data.npy', data_to_save)
print("Saved generation data for future use")

class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(
            name="attention_weight",
            shape=(input_shape[-1], 1),
            initializer="normal"
        )
        self.b = self.add_weight(
            name="attention_bias",
            shape=(input_shape[1], 1),
            initializer="zeros"
        )
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        e = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)
        a = tf.nn.softmax(e, axis=1)
        output = inputs * a
        return tf.reduce_sum(output, axis=1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[2])

    def get_config(self):
        return super(AttentionLayer, self).get_config()

def build_model(x_shape, num_classes):
    print("Building enhanced RNN+Attention model...")
    print(f"Input shape: {x_shape}, Output classes: {num_classes}")

    inputs = Input(shape=(x_shape[1], x_shape[2]))

    rnn1 = Bidirectional(GRU(256, return_sequences=True))(inputs)
    dropout1 = Dropout(0.3)(rnn1)

    rnn2 = Bidirectional(GRU(256, return_sequences=True))(dropout1)
    dropout2 = Dropout(0.3)(rnn2)

    attention = AttentionLayer()(dropout2)

    dense1 = Dense(512, activation='relu')(attention)
    dropout3 = Dropout(0.3)(dense1)

    dense2 = Dense(256, activation='relu')(dropout3)
    dropout4 = Dropout(0.2)(dense2)

    outputs = Dense(num_classes, activation='softmax')(dropout4)

    model = Model(inputs=inputs, outputs=outputs)

    model.compile(loss='categorical_crossentropy',
                 optimizer=Adam(learning_rate=0.001),
                 metrics=['accuracy'])

    model.summary()

    tf.keras.utils.plot_model(
        model,
        to_file='model_architecture.png',
        show_shapes=True,
        show_layer_names=True,
        rankdir='TB'
    )

    print("Model architecture visualization saved to 'model_architecture.png'")

    display(plt.imread('model_architecture.png'))

    print("Model construction complete.")
    print(f"- Input shape: {x_shape}")
    print(f"- Output classes: {num_classes}")
    print(f"- Bidirectional GRU layers with Attention mechanism")
    print(f"- Learning rate: 0.001")

    return model

model = build_model(x.shape, len(pitchnames))

def train_model(model, x, y, epochs=30, batch_size=256, steps_per_epoch=200):
    print("Preparing for model training...")
    print(f"Training configuration:")
    print(f"- Epochs: {epochs}")
    print(f"- Batch size: {batch_size}")
    print(f"- Steps per epoch: {steps_per_epoch}")
    print(f"- Training samples: {len(x)}")

    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    print("Dataset prepared with shuffling and prefetching for optimal performance.")

    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='accuracy',
        patience=5,
        restore_best_weights=True,
        mode='max',
        verbose=1
    )

    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='loss',
        factor=0.5,
        patience=3,
        min_lr=0.0001,
        verbose=1
    )

    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        filepath="best_model.keras",
        monitor='accuracy',
        save_best_only=True,
        save_weights_only=False,
        mode='max',
        verbose=1
    )

    tensorboard = tf.keras.callbacks.TensorBoard(
        log_dir='./logs',
        histogram_freq=1,
        write_graph=True,
        update_freq='epoch'
    )

    print("Starting model training...")

    start_time = tf.timestamp()

    history = model.fit(
        dataset,
        epochs=epochs,
        steps_per_epoch=steps_per_epoch,
        callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard],
        verbose=1
    )

    end_time = tf.timestamp()
    training_time = end_time - start_time

    print(f"Training completed in {training_time.numpy():.2f} seconds")

    try:
        model = tf.keras.models.load_model(
            "best_model.keras",
            custom_objects={"AttentionLayer": AttentionLayer}
        )
        print("Loaded best model from checkpoint")
    except Exception as e:
        print(f"Could not load best model: {e}")

    return model, history

def plot_training_history(history):
    plt.figure(figsize=(15, 6))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    if 'lr' in history.history:
        plt.figure(figsize=(10, 4))
        plt.plot(history.history['lr'], marker='o')
        plt.title('Learning Rate over Time')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.grid(True)
        plt.show()

    print("Training performance metrics:")
    print(f"- Final loss: {history.history['loss'][-1]:.4f}")
    print(f"- Final accuracy: {history.history['accuracy'][-1]:.4f}")
    print(f"- Best accuracy: {max(history.history['accuracy']):.4f}")
    print(f"- Starting loss: {history.history['loss'][0]:.4f}")
    print(f"- Starting accuracy: {history.history['accuracy'][0]:.4f}")
    print(f"- Total epochs trained: {len(history.history['loss'])}")

print("Starting model training...")
trained_model, history = train_model(
    model,
    x,
    y,
    epochs=30,
    batch_size=256,
    steps_per_epoch=200
)

print("Training completed, plotting results...")
plot_training_history(history)

def generate_notes(model, input_sequence, int_to_note, pitchnames, num_notes=500, temperature=1.0):
    print(f"Generating {num_notes} notes with temperature {temperature}...")

    pattern = list(input_sequence)
    generated_notes = []

    for i in tqdm(range(num_notes), desc="Generating notes"):
        x_pred = np.array([pattern])
        x_pred = x_pred / float(len(pitchnames))
        x_pred = x_pred.reshape(1, len(pattern), 1)

        prediction = model.predict(x_pred, verbose=0)[0]

        prediction = np.log(prediction + 1e-10) / temperature
        exp_preds = np.exp(prediction)
        prediction = exp_preds / np.sum(exp_preds)

        index = np.random.choice(len(prediction), p=prediction)
        note_result = int_to_note[index]

        generated_notes.append(note_result)

        pattern.append(index)
        pattern = pattern[1:]

        if i % 100 == 0 and i > 0:
            print(f"Generated {i} notes so far...")

    print(f"Generated {len(generated_notes)} notes successfully.")

    note_counts = Counter(generated_notes)
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    top_notes = dict(note_counts.most_common(10))
    plt.bar(top_notes.keys(), top_notes.values())
    plt.title('10 Most Common Generated Notes')
    plt.xlabel('MIDI Note')
    plt.ylabel('Count')
    plt.xticks(rotation=45)

    plt.subplot(1, 2, 2)
    plt.hist([int(n) for n in generated_notes], bins=20, color='skyblue', edgecolor='black')
    plt.title('Distribution of Generated Notes')
    plt.xlabel('MIDI Note Value')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

    return generated_notes

def create_midi(generated_notes, output_path='generated_music.mid'):
    print(f"Creating MIDI file at {output_path}...")

    output_notes = stream.Stream()

    for pattern in tqdm(generated_notes, desc="Creating MIDI"):
        try:
            n = note.Note(int(pattern))
            n.quarterLength = 0.5
            output_notes.append(n)
        except:
            continue

    output_notes.write('midi', fp=output_path)
    print(f"MIDI file saved to {output_path}")
    return output_path

def play_midi_in_colab(midi_path):
    print(f"Setting up playback for {midi_path}...")

    try:
        from midi2audio import FluidSynth

        wav_path = midi_path.replace('.mid', '.wav')
        print(f"Converting MIDI to audio at {wav_path}...")

        fs = FluidSynth()
        fs.midi_to_audio(midi_path, wav_path)

        print("Playing audio...")
        display(Audio(wav_path, autoplay=True))

        return True
    except Exception as e:
        print(f"Error setting up MIDI playback: {e}")
        return False

start_index = np.random.randint(0, len(notes) - sequence_length - 1)
input_sequence = [note_to_int[notes[i]] for i in range(start_index, start_index + sequence_length)]

print("Creating new musical composition...")
generated_notes = generate_notes(
    trained_model,
    input_sequence,
    int_to_note,
    pitchnames,
    num_notes=500,
    temperature=1.0
)

output_path = 'generated_classical_music.mid'
midi_path = create_midi(generated_notes, output_path)

play_midi_in_colab(midi_path)

files.download(midi_path)
print(f"Music generation complete. File available for download: {midi_path}")

def generate_new_music(num_notes=500, temperature=1.0, output_name=None):
    print(f"Generating new music with {num_notes} notes and temperature {temperature}...")

    try:
        print("Loading model from best checkpoint...")
        model = tf.keras.models.load_model(
            "best_model.keras",
            custom_objects={'AttentionLayer': AttentionLayer}
        )
        print("Model loaded successfully")
    except:
        try:
            print("Trying to load model from H5 format...")
            model = tf.keras.models.load_model(
                "best_model.h5",
                custom_objects={'AttentionLayer': AttentionLayer}
            )
            print("Model loaded from H5 format")
        except Exception as e:
            print(f"Error loading model: {e}")
            return None

    try:
        print("Loading generation data...")
        loaded_data = np.load('generation_data.npy', allow_pickle=True).item()
        int_to_note = loaded_data['int_to_note']
        notes = loaded_data['notes']
        note_to_int = loaded_data['note_to_int']
        pitchnames = loaded_data['pitchnames']
        sequence_length = loaded_data['sequence_length']
        print(f"Data loaded with {len(pitchnames)} unique notes and sequence length {sequence_length}")
    except Exception as e:
        print(f"Error loading generation data: {e}")
        return None

    print("Creating random seed sequence...")
    start_index = random.randint(0, len(notes) - sequence_length - 1)
    input_sequence = [note_to_int[notes[i]] for i in range(start_index, start_index + sequence_length)]

    print(f"Generating {num_notes} new notes...")
    pattern = list(input_sequence)
    generated_notes = []

    for _ in tqdm(range(num_notes), desc="Generating notes"):
        x_pred = np.array([pattern])
        x_pred = x_pred / float(len(pitchnames))
        x_pred = x_pred.reshape(1, len(pattern), 1)

        prediction = model.predict(x_pred, verbose=0)[0]

        prediction = np.log(prediction + 1e-10) / temperature
        exp_preds = np.exp(prediction)
        prediction = exp_preds / np.sum(exp_preds)

        index = np.random.choice(len(prediction), p=prediction)
        note_result = int_to_note[index]

        generated_notes.append(note_result)

        pattern.append(index)
        pattern = pattern[1:]

    print(f"Generated {len(generated_notes)} notes.")

    if output_name is None:
        output_name = f"generated_music_{random.randint(1000, 9999)}.mid"

    print(f"Creating MIDI file: {output_name}")
    output_notes = stream.Stream()

    for pattern in tqdm(generated_notes, desc="Creating MIDI"):
        try:
            n = note.Note(int(pattern))
            n.quarterLength = 0.5
            output_notes.append(n)
        except:
            continue

    output_notes.write('midi', fp=output_name)

    print("Visualizing generated music patterns...")
    note_counts = Counter(generated_notes)
    plt.figure(figsize=(15, 6))

    plt.subplot(1, 3, 1)
    top_notes = dict(note_counts.most_common(10))
    plt.bar(top_notes.keys(), top_notes.values())
    plt.title('Most Common Generated Notes')
    plt.xlabel('MIDI Note')
    plt.ylabel('Count')
    plt.xticks(rotation=45)

    plt.subplot(1, 3, 2)
    plt.hist([int(n) for n in generated_notes], bins=20, color='skyblue', edgecolor='black')
    plt.title('Distribution of Generated Notes')
    plt.xlabel('MIDI Note Value')
    plt.ylabel('Frequency')

    plt.subplot(1, 3, 3)
    note_sequence = [int(n) for n in generated_notes[:100]]
    plt.plot(note_sequence)
    plt.title('Melody Contour (First 100 Notes)')
    plt.xlabel('Note Position')
    plt.ylabel('MIDI Note Value')

    plt.tight_layout()
    plt.show()

    print(f"Playing generated music: {output_name}")
    try:
        from midi2audio import FluidSynth
        wav_name = output_name.replace('.mid', '.wav')
        fs = FluidSynth()
        fs.midi_to_audio(output_name, wav_name)
        display(Audio(wav_name, autoplay=True))
    except Exception as e:
        print(f"Couldn't play audio: {e}")

    files.download(output_name)
    print(f"Generation complete! File available for download: {output_name}")

    return output_name

def generate_variations(base_count=3, temperatures=[0.7, 1.0, 1.3]):
    print(f"Generating {len(temperatures)} musical variations with different temperatures")

    for i, temp in enumerate(temperatures):
        output_name = f"classical_variation_{i+1}_temp{temp}.mid"
        print(f"\nVariation {i+1}/{len(temperatures)} - Temperature: {temp}")
        generate_new_music(num_notes=500, temperature=temp, output_name=output_name)

    print("\nAll variations generated successfully!")

print("Run this cell anytime to generate new classical music!")
print("Option 1: Generate a single piece of music")
print("Option 2: Generate multiple variations with different temperatures")

option = 1  # Change to 2 for multiple variations

if option == 1:
    output_path = generate_new_music(num_notes=500, temperature=1.0)
    print(f"Music generated at: {output_path}")
else:
    generate_variations(base_count=3, temperatures=[0.7, 1.0, 1.3])